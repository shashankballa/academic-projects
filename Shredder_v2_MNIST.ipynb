{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM3YeOQwKcg5D65b4B0DFGk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shashankballa/academic-projects/blob/master/Shredder_v2_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du91C_HbknZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# # Extract conv\n",
        "\n",
        "# In[8]:\n",
        "\n",
        "\n",
        "conv_shapes=[]\n",
        "for cnt2, (data, target) in enumerate(data_test_loader):\n",
        "    for cnt,i in enumerate(conv_layers):\n",
        "        #newmodel = torch.nn.Sequential(*(list(model_test.features)[0:i]))\n",
        "        newmodel_original =  torch.nn.Sequential(*(list(model_original.convnet)[0:i]))\n",
        "        \n",
        "        output_original = newmodel_original(data)\n",
        "        conv_shapes.append(output_original.shape[1:])\n",
        "        print (output_original.shape[1:])\n",
        "    if (cnt2==0):\n",
        "        break\n",
        "    \n",
        "\n",
        "\n",
        "# # build new model\n",
        "\n",
        "# In[9]:\n",
        "\n",
        "\n",
        "class NoisyActivation(nn.Module):\n",
        "    def __init__(self, activation_size):\n",
        "        super(NoisyActivation, self).__init__()\n",
        "        \n",
        "        m =torch.distributions.laplace.Laplace(loc = 0.0, scale = 20.0, validate_args=None)\n",
        "        self.noise = nn.Parameter(m.rsample(activation_size))\n",
        "        #self.noise = nn.Parameter(torch.Tensor(activation_size).normal_(loc=0.0, scale=12.0))\n",
        "        self.weight = nn.Parameter(torch.Tensor(activation_size))\n",
        "        nn.init.xavier_normal_(self.weight)\n",
        "        \n",
        "    def forward(self, input):\n",
        "\n",
        "        return input*self.weight + self.noise\n",
        "\n",
        "\n",
        "# In[10]:\n",
        "\n",
        "\n",
        "\n",
        "class LeNet_syn(nn.Module):\n",
        "\n",
        "    def __init__(self, model_features, model_classifier, conv_layers, conv_shapes, index ):\n",
        "        super(LeNet_syn, self).__init__()\n",
        "        \n",
        "        self.model_pt1 =  torch.nn.Sequential(*(list(model_features)[0:conv_layers[index]]))\n",
        "        self.intermed = NoisyActivation(conv_shapes[index])\n",
        "        self.model_pt2 =  torch.nn.Sequential(*(list(model_features)[conv_layers[index]:]))\n",
        "        self.model_pt3 = model_classifier\n",
        "        for child in itertools.chain(self.model_pt1,self.model_pt2,self.model_pt3):\n",
        "            for param in child.parameters():\n",
        "                param.requires_grad = False\n",
        "            if isinstance(child, nn.modules.batchnorm._BatchNorm):\n",
        "                child.eval()\n",
        "                child.affine = False\n",
        "                child.track_running_stats = False\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.model_pt1(img)\n",
        "        x = self.intermed (x)\n",
        "        x = self.model_pt2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.model_pt3(x)\n",
        "\n",
        "        return x\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "model_syn = LeNet_syn(model_original.convnet, model_original.fc ,conv_layers, conv_shapes, 2)\n",
        "\n",
        "\n",
        "# In[12]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "total_correct = 0\n",
        "avg_loss = 0.0\n",
        "for i, (images, labels) in enumerate(data_test_loader):\n",
        "    output = model_syn(images)\n",
        "    labels = (labels > 5).long()\n",
        "    avg_loss += criterion(output, labels).sum()\n",
        "    pred = output.detach().max(1)[1]\n",
        "    total_correct += pred.eq(labels.view_as(pred)).sum()\n",
        "end=time.perf_counter()\n",
        "\n",
        "avg_loss /= len(data_test)\n",
        "print('Test Avg. Loss: %f, Accuracy: %f' % (avg_loss.detach().cpu().item(), float(total_correct) / len(data_test)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model_syn.parameters()), lr=0.001, weight_decay=-0.01)\n",
        "weights_noise =np.expand_dims( model_syn.intermed.noise.detach().numpy(),axis=0)\n",
        "weights_weight =np.expand_dims( model_syn.intermed.weight.detach().numpy(),axis=0)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "size = len(data_train_loader_2)\n",
        "\n",
        "\n",
        "size2 = len(data_test_loader)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_above_5(net, optimizer, epoch):\n",
        "    net.train()\n",
        "    avg_loss = 0\n",
        "    total_correct=0\n",
        "    for i, (images, labels) in enumerate(data_train_loader):\n",
        "        if (i == size-1):\n",
        "            continue\n",
        "            \n",
        "            \n",
        "        net.zero_grad()\n",
        "        b_size = images.shape[0]\n",
        "        #print(b_size)\n",
        "        index_rand = np.random.randint(1,size-1)\n",
        "        (images_2,labels_2) = data_train_loader_2[index_rand]\n",
        "        #print(\"img size\", images_2.shape[0], index_rand)\n",
        "        \n",
        "        ####################################  self sup\n",
        "        labels = (labels > 5).long()\n",
        "        labels_2 = (labels_2 > 5).long()\n",
        "        \n",
        "        output_main = net.model_pt1(images)\n",
        "        output_main = net.intermed(output_main)\n",
        "        \n",
        "        output_rand = net.model_pt1(images_2)\n",
        "        output_rand = net.intermed(output_rand)\n",
        "        \n",
        "        distance = abs(output_rand-output_main)\n",
        "        distance = torch.sum(distance, dim = 1)\n",
        "        distance = distance.squeeze()\n",
        "        pos = (labels == labels_2)\n",
        "        neg = (labels != labels_2)\n",
        "        pos = torch.sum(distance * pos.type(torch.FloatTensor))\n",
        "        neg = torch.sum(distance * neg.type(torch.FloatTensor))\n",
        "        #print(output_rand.shape)\n",
        "        \n",
        "        output = net(images)\n",
        "\n",
        "        \n",
        "        all = pos+ neg\n",
        "        ######################################## calculate distribution distance \n",
        "        #params = st.norm.fit(model_syn.intermed.weight.detach().numpy())\n",
        "        #arg = params[:-2]\n",
        "        #loc = params[-2]\n",
        "        #scale = params[-1]\n",
        "        #dist = st.norm(loc, scale)\n",
        "        #y, x = np.histogram(model_syn.intermed.weight.detach().numpy(), bins=200, density=True)\n",
        "        #x = (x + np.roll(x, -1))[:-1] / 2.0\n",
        "        \n",
        "        #pdf = st.laplace.pdf(x, loc=loc, scale=scale, *arg)\n",
        "        #sse = np.sum(np.power(y - pdf, 2.0))\n",
        "        #print(sse, \"SSE\")\n",
        "        \n",
        "        ########################################\n",
        "        \n",
        "        #print(pos, \"pos\")\n",
        "        #print(neg,\"neg\")\n",
        "        #print ((pos - neg)/all , \"norm\")\n",
        "        loss = criterion(output, labels) + 0.01*(pos - neg)  #+ 0.01*sse\n",
        "        \n",
        "        #if ()\n",
        "        avg_loss += loss \n",
        "        pred = output.detach().max(1)[1]\n",
        "        total_correct += pred.eq(labels.view_as(pred)).sum()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print(\"here\")\n",
        "    avg_loss /= len(data_train)\n",
        "    print('Train Avg. Loss: %f, Accuracy: %f' % (avg_loss.detach().cpu().item(), float(total_correct) / len(data_train)))\n",
        "    return float(float(total_correct) / len(data_train))\n",
        "\n",
        "\n",
        "# In[107]:\n",
        "\n",
        "\n",
        "def validate(model_syn, index):\n",
        "    model_syn.eval()\n",
        "    total_correct = 0\n",
        "    avg_loss = 0.0\n",
        "    SNR =[]\n",
        "    for i, (images, labels) in enumerate(data_test_loader):\n",
        "        output = model_syn(images)\n",
        "        avg_loss += criterion_5(output, labels).sum()\n",
        "        pred = output.detach().max(1)[1]\n",
        "        total_correct += pred.eq(labels.view_as(pred)).sum()\n",
        "\n",
        "        output_original = model_original.convnet[0:conv_layers[index]](data)\n",
        "        output_syn = model_syn.model_pt1(data)\n",
        "        output_syn= model_syn.intermed(output_syn)\n",
        "\n",
        "        \n",
        "        SNR.append((((output_original**2).mean())/((output_original-output_syn).var())).item())\n",
        "\n",
        "    print(\"Avg SNR\", sum(SNR)/len(SNR))\n",
        "    avg_loss /= len(data_test)\n",
        "    print('Test Avg. Loss: %f, Accuracy: %f' % (avg_loss.detach().cpu().item(), float(total_correct) / len(data_test)))\n",
        "    return  float(total_correct) / len(data_test), sum(SNR)/len(SNR)\n",
        "\n",
        "\n",
        "# In[108]:\n",
        "\n",
        "\n",
        "def validate_above_5(model_syn, index):\n",
        "    model_syn.eval()\n",
        "    total_correct = 0\n",
        "    avg_loss = 0.0\n",
        "    SNR =[]\n",
        "    pos_s = 0\n",
        "    neg_s =0\n",
        "    norm_s =0\n",
        "    for i, (images, labels) in enumerate(data_test_loader):\n",
        "\n",
        "        \n",
        "        index_rand = np.random.randint(1,size2-1)\n",
        "        if (i == size2-1 and index_rand != size2-1):\n",
        "            b_size = labels.shape[0]\n",
        "            (images_2,labels_2) = data_test_loader_2[index_rand]\n",
        "            (images_2,labels_2) = ((images_2[0:b_size],labels_2[0:b_size]) )\n",
        "        \n",
        "        else:\n",
        "            (images_2,labels_2) = data_test_loader_2[index_rand]\n",
        "        #print(\"img size\", images_2.shape[0], index_rand)\n",
        "        labels = (labels > 5).long()\n",
        "        labels_2 = (labels_2 > 5).long()\n",
        "        \n",
        "        output_main = model_syn.model_pt1(images)\n",
        "        output_main = model_syn.intermed(output_main)\n",
        "        \n",
        "        output_rand = model_syn.model_pt1(images_2)\n",
        "        output_rand = model_syn.intermed(output_rand)\n",
        "        \n",
        "        distance = abs(output_rand-output_main)\n",
        "        distance = torch.sum(distance, dim = 1)\n",
        "        distance = distance.squeeze()\n",
        "        pos = (labels == labels_2)\n",
        "        neg = (labels != labels_2)\n",
        "        pos = torch.sum(distance * pos.type(torch.FloatTensor))\n",
        "        neg = torch.sum(distance * neg.type(torch.FloatTensor))\n",
        "        #print(output_rand.shape)\n",
        "        pos_s += pos\n",
        "        neg_s += neg\n",
        "      \n",
        "        #######################################\n",
        "        output = model_syn(images)\n",
        "\n",
        "        avg_loss += criterion(output, labels).sum()\n",
        "        pred = output.detach().max(1)[1]\n",
        "        total_correct += pred.eq(labels.view_as(pred)).sum()\n",
        "\n",
        "        output_original = model_original.convnet[0:conv_layers[index]](data)\n",
        "        output_syn = model_syn.model_pt1(data)\n",
        "        output_syn= model_syn.intermed(output_syn)\n",
        "\n",
        "\n",
        "        SNR.append((((output_original**2).mean())/((output_original-output_syn).var())).item())\n",
        "    print(\"AVG SNR\", sum(SNR)/len(SNR) )\n",
        "    avg_loss /= len(data_test)\n",
        "    print('Test Avg. Loss: %f, Accuracy: %f' % (avg_loss.detach().cpu().item(), float(total_correct) / len(data_test)))\n",
        "    return  float(total_correct) / len(data_test), sum(SNR)/len(SNR)\n",
        "\n",
        "\n",
        "# In[109]:\n",
        "\n",
        "\n",
        "#print (list(synthesized.sequential_list[0].parameters()))\n",
        "deltas = []\n",
        "\n",
        "for run in range (1000):\n",
        "    wd =0\n",
        "    lr = 0.001\n",
        "    model_original = LeNet5_5()\n",
        "    model_original.load_state_dict(torch.load(\"LeNet-5-saved\"))\n",
        "    model_syn = LeNet_syn(model_original.convnet, model_original.fc ,conv_layers, conv_shapes, 2)\n",
        "    model_syn.eval()\n",
        "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model_syn.parameters()), lr=lr, weight_decay=wd)\n",
        "    acc_shadow = 100.0\n",
        "\n",
        "    for epoch in range(40):\n",
        "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model_syn.parameters()), lr=lr, weight_decay=wd)\n",
        "        print (epoch)\n",
        "        acc, SNR =validate_above_5 (model_syn,2)\n",
        "        \n",
        "        if (acc > 0.95 and epoch>27):\n",
        "            #break\n",
        "            print (\"saved\")\n",
        "            with open('self-super-std20-nonsen-27ep-2-new.csv','a') as fd:\n",
        "                    writer = csv.writer(fd)\n",
        "                    writer.writerow([SNR, acc,epoch])\n",
        "            print (weights_noise.shape)\n",
        "            print (np.expand_dims(model_syn.intermed.noise.detach().numpy(),axis=0).shape)\n",
        "            \n",
        "            weights_noise=np.concatenate((weights_noise,np.expand_dims(model_syn.intermed.noise.detach().numpy(),axis=0)),axis=0)\n",
        "            weights_weight=np.concatenate((weights_weight, np.expand_dims(model_syn.intermed.weight.detach().numpy(),axis=0)),axis=0)\n",
        "\n",
        "     \n",
        "            np.save(\"self-super-std20-nonsen-27ep-noise-2-new\", weights_noise)\n",
        "            np.save(\"self-super-std20-nonsen-27ep-weight-2-new\", weights_weight)\n",
        "            \n",
        "            break\n",
        "        train_above_5(model_syn, optimizer, epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c87hqXooI7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Email fmireshg@eng.ucsd.edu fatamehsadat Mireshghallah in case of questions\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from lenet import LeNet5\n",
        "from lenet_5 import LeNet5_5\n",
        "from torchvision.datasets.mnist import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "#from tqdm import tqdm, trange\n",
        "import math\n",
        "import csv\n",
        "import numpy as np\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import scipy.stats as st"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m4C5bReo_T8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "BATCH_TEST_SIZE = 1024\n",
        "data_train = MNIST('./data/mnist',\n",
        "                   download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.Resize((32, 32)),\n",
        "                       transforms.ToTensor()]))\n",
        "data_test = MNIST('./data/mnist',\n",
        "                  train=False,\n",
        "                  download=True,\n",
        "                  transform=transforms.Compose([\n",
        "                      transforms.Resize((32, 32)),\n",
        "                      transforms.ToTensor()]))\n",
        "data_train_loader = DataLoader(data_train, batch_size = BATCH_SIZE , shuffle=True, num_workers=8)\n",
        "data_test_loader = DataLoader(data_test,  batch_size = BATCH_TEST_SIZE, num_workers=8)\n",
        "\n",
        "\n",
        "# Why ?\n",
        "data_train_loader_2 = list(DataLoader(data_train, batch_size = BATCH_SIZE , shuffle=True, num_workers=0))\n",
        "data_test_loader_2 = list(DataLoader(data_test,  batch_size = BATCH_TEST_SIZE, num_workers=0))\n",
        "\n",
        "TRAIN_SIZE = len(data_train_loader.dataset)\n",
        "TEST_SIZE = len(data_test_loader.dataset)\n",
        "NUM_BATCHES = len(data_train_loader)\n",
        "NUM_TEST_BATCHES = len(data_test_loader)\n",
        "\n",
        "CLASSES = 10\n",
        "TRAIN_EPOCHS = 10\n",
        "SAMPLES = 2\n",
        "TEST_SAMPLES = 10\n",
        "SAMPLE = True\n",
        "LR = 0.001\n",
        "COEF = 0.1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACTRC3BPqYX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_original = LeNet5_5()\n",
        "model_original.load_state_dict(torch.load(\"LeNet-5-saved\"))\n",
        "\n",
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6E7144HrPLN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e82df4b6-4051-489b-f08f-daa9ca148a78"
      },
      "source": [
        "model_original.eval()\n",
        "total_correct = 0\n",
        "avg_loss = 0.0\n",
        "for i, (images, labels) in enumerate(data_test_loader):\n",
        "    output = model_original(images)\n",
        "    labels = (labels > 5).long()\n",
        "    avg_loss += criterion(output, labels).sum()\n",
        "    pred = output.detach().max(1)[1]\n",
        "    total_correct += pred.eq(labels.view_as(pred)).sum()\n",
        "end=time.perf_counter()\n",
        "\n",
        "avg_loss /= len(data_test)\n",
        "print('Test Avg. Loss: %f, Accuracy: %f' % (avg_loss.detach().cpu().item(), float(total_correct) / len(data_test)))\n",
        "\n",
        "\n",
        "conv_layers = []\n",
        "fc_layers=[]\n",
        "for i, layer in enumerate(model_original.convnet):\n",
        "    if isinstance(layer, nn.Conv2d):\n",
        "        if ((i is not 0 )):\n",
        "            conv_layers.append(i)\n",
        "conv_layers.append(len(model_original.convnet))\n",
        "for i, layer in enumerate(model_original.fc):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        fc_layers.append(i)\n",
        "\n",
        "print (conv_layers, fc_layers)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Avg. Loss: 0.000029, Accuracy: 0.990900\n",
            "[3, 6, 8] [0, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSZ0uTAVsqJh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e61273d6-5bf8-449e-98d9-4184d078206b"
      },
      "source": [
        "# # Extract conv\n",
        "\n",
        "conv_shapes=[]\n",
        "for cnt2, (data, target) in enumerate(data_test_loader):\n",
        "    for cnt,i in enumerate(conv_layers):\n",
        "        #newmodel = torch.nn.Sequential(*(list(model_test.features)[0:i]))\n",
        "        newmodel_original =  torch.nn.Sequential(*(list(model_original.convnet)[0:i]))\n",
        "        \n",
        "        output_original = newmodel_original(data)\n",
        "        conv_shapes.append(output_original.shape[1:])\n",
        "        print (output_original.shape[1:])\n",
        "    if (cnt2==0):\n",
        "        break"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([6, 14, 14])\n",
            "torch.Size([16, 5, 5])\n",
            "torch.Size([120, 1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kae3ODnQN0a5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Build New Model\n",
        "\n",
        "class NoisyActivation(nn.Module):\n",
        "    def __init__(self, activation_size):\n",
        "        super(NoisyActivation, self).__init__()\n",
        "        \n",
        "        m =torch.distributions.laplace.Laplace(loc = 0.0, scale = 20.0, validate_args=None)\n",
        "        self.noise = nn.Parameter(m.rsample(activation_size))\n",
        "        #self.noise = nn.Parameter(torch.Tensor(activation_size).normal_(loc=0.0, scale=12.0))\n",
        "        self.weight = nn.Parameter(torch.Tensor(activation_size))\n",
        "        nn.init.xavier_normal_(self.weight)\n",
        "        \n",
        "    def forward(self, input):\n",
        "\n",
        "        return input*self.weight + self.noise"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kAHZ53ESnjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LeNet_syn(nn.Module):\n",
        "\n",
        "    def __init__(self, model_features, model_classifier, conv_layers, conv_shapes, index ):\n",
        "        super(LeNet_syn, self).__init__()\n",
        "        \n",
        "        self.model_pt1 =  torch.nn.Sequential(*(list(model_features)[0:conv_layers[index]]))\n",
        "        self.intermed = NoisyActivation(conv_shapes[index])\n",
        "        self.model_pt2 =  torch.nn.Sequential(*(list(model_features)[conv_layers[index]:]))\n",
        "        self.model_pt3 = model_classifier\n",
        "        for child in itertools.chain(self.model_pt1,self.model_pt2,self.model_pt3):\n",
        "            for param in child.parameters():\n",
        "                param.requires_grad = False\n",
        "            if isinstance(child, nn.modules.batchnorm._BatchNorm):\n",
        "                child.eval()\n",
        "                child.affine = False\n",
        "                child.track_running_stats = False\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.model_pt1(img)\n",
        "        x = self.intermed (x)\n",
        "        x = self.model_pt2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.model_pt3(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RDowDBwV-jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_syn = LeNet_syn(model_original.convnet, model_original.fc ,conv_layers, conv_shapes, 2)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1UXhz7iXK-1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "21033627-310f-4ba4-c41a-00c282f50f35"
      },
      "source": [
        "total_correct = 0\n",
        "avg_loss = 0.0\n",
        "for i, (images, labels) in enumerate(data_test_loader):\n",
        "    output = model_syn(images)\n",
        "    labels = (labels > 5).long()\n",
        "    avg_loss += criterion(output, labels).sum()\n",
        "    pred = output.detach().max(1)[1]\n",
        "    total_correct += pred.eq(labels.view_as(pred)).sum()\n",
        "end=time.perf_counter()\n",
        "\n",
        "avg_loss /= len(data_test)\n",
        "print('Test Avg. Loss: %f, Accuracy: %f' % (avg_loss.detach().cpu().item(), float(total_correct) / len(data_test)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Avg. Loss: 0.023229, Accuracy: 0.603100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQhb40MhZPqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model_syn.parameters()), lr=0.001, weight_decay=0.01)\n",
        "weights_noise =np.expand_dims( model_syn.intermed.noise.detach().numpy(),axis=0)\n",
        "weights_weight =np.expand_dims( model_syn.intermed.weight.detach().numpy(),axis=0)\n",
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvrHzV12WEcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "size = len(data_train_loader_2)\n",
        "size2 = len(data_test_loader)\n",
        "\n",
        "def train_above_5(net, optimizer, epoch):\n",
        "    net.train()\n",
        "    avg_loss = 0\n",
        "    total_correct=0\n",
        "    for i, (images, labels) in enumerate(data_train_loader):\n",
        "        if (i == size-1):\n",
        "            continue\n",
        "        net.zero_grad()\n",
        "        b_size = images.shape[0]\n",
        "        #print(b_size)\n",
        "        index_rand = np.random.randint(1,size-1)\n",
        "        (images_2,labels_2) = data_train_loader_2[index_rand]\n",
        "        #print(\"img size\", images_2.shape[0], index_rand)\n",
        "        \n",
        "        ####################################  self sup\n",
        "        labels = (labels > 5).long()\n",
        "        labels_2 = (labels_2 > 5).long()\n",
        "        \n",
        "        output_main = net.model_pt1(images)\n",
        "        output_main = net.intermed(output_main)\n",
        "        \n",
        "        output_rand = net.model_pt1(images_2)\n",
        "        output_rand = net.intermed(output_rand)\n",
        "        \n",
        "        distance = abs(output_rand-output_main)\n",
        "        distance = torch.sum(distance, dim = 1)\n",
        "        distance = distance.squeeze()\n",
        "        pos = (labels == labels_2)\n",
        "        neg = (labels != labels_2)\n",
        "        pos = torch.sum(distance * pos.type(torch.FloatTensor))\n",
        "        neg = torch.sum(distance * neg.type(torch.FloatTensor))\n",
        "        #print(output_rand.shape)\n",
        "        \n",
        "        output = net(images)\n",
        "\n",
        "        \n",
        "        all = pos+ neg\n",
        "        ######################################## calculate distribution distance \n",
        "        #params = st.norm.fit(model_syn.intermed.weight.detach().numpy())\n",
        "        #arg = params[:-2]\n",
        "        #loc = params[-2]\n",
        "        #scale = params[-1]\n",
        "        #dist = st.norm(loc, scale)\n",
        "        #y, x = np.histogram(model_syn.intermed.weight.detach().numpy(), bins=200, density=True)\n",
        "        #x = (x + np.roll(x, -1))[:-1] / 2.0\n",
        "        \n",
        "        #pdf = st.laplace.pdf(x, loc=loc, scale=scale, *arg)\n",
        "        #sse = np.sum(np.power(y - pdf, 2.0))\n",
        "        #print(sse, \"SSE\")\n",
        "        \n",
        "        ########################################\n",
        "        \n",
        "        #print(pos, \"pos\")\n",
        "        #print(neg,\"neg\")\n",
        "        #print ((pos - neg)/all , \"norm\")\n",
        "        loss = criterion(output, labels) + 0.01*(pos - neg)  #+ 0.01*sse\n",
        "        \n",
        "        #if ()\n",
        "        avg_loss += loss \n",
        "        pred = output.detach().max(1)[1]\n",
        "        total_correct += pred.eq(labels.view_as(pred)).sum()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print(\"here\")\n",
        "    avg_loss /= len(data_train)\n",
        "    print('Train Avg. Loss: %f, Accuracy: %f' % (avg_loss.detach().cpu().item(), float(total_correct) / len(data_train)))\n",
        "    return float(float(total_correct) / len(data_train))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}